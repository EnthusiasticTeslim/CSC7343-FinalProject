{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.35.2'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model import BERT_CONFIG, FocalLoss, TCRModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.sampler import SubsetRandomSample\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "class ModelTrainer(TCRModel):\n",
    "    '''\n",
    "        ref: https://github.com/EnthusiasticTeslim/PianoGen/blob/master/hw1.py\n",
    "        '''\n",
    "    \n",
    "    def __init__(self, args, train=False, seed = 2023, lr=2e-5, epochs=1000, log_interval=200, verbose=True, model_dir='model_save'):\n",
    "        \n",
    "        self.seed = seed # seed for random number generator\n",
    "        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=args.lr)\n",
    "        self.epochs = epochs # number of epochs to train\n",
    "        self.log_interval = log_interval # how many batches to wait before logging training status\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else (\"mps\" if torch.backends.mps.is_available() else \"cpu\"))\n",
    "        self.model = TCRModel.to(self.device) # takes input_ids, attention_mask, classification\n",
    "\n",
    "    def train(self, train_loader):\n",
    "        \n",
    "        # set the seed for generating random numbers\n",
    "        torch.manual_seed(args.seed)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed(args.seed)\n",
    "        \n",
    "\n",
    "        for epoch in range(0, self.epochs + 1):\n",
    "            self.model.train()\n",
    "            for step, batch in enumerate(train_loader):\n",
    "\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                input_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['targets'].to(device)\n",
    "\n",
    "                outputs = self.model(\n",
    "                                    input_ids = input_ids  # amino acid index numbers\n",
    "                                    attention_mask = input_mask, # attention mask (1 for non-padding token and 0 for padding)\n",
    "                                    classification = True # True for classification task\n",
    "                                    )\n",
    "                self.model.to(self.device)\n",
    "                loss = clf_loss_func(input=outputs, target=labels)\n",
    "\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                self.optimizer.zero_grad()\n",
    "                \n",
    "                if step % self.log_interval == 0:\n",
    "                    logger.info(\n",
    "                        \"Train Epoch: {} [{}/{} ({:.0f}%)] Training Loss: {:.6f}\".format(\n",
    "                            epoch,\n",
    "                            step * len(batch['input_ids'])*world_size,\n",
    "                            len(train_loader.dataset),\n",
    "                            100.0 * step / len(train_loader),\n",
    "                            loss\n",
    "                        )\n",
    "                    )\n",
    "        \n",
    "        def test(self, test_loader):\n",
    "            \n",
    "            self.model.eval()\n",
    "            sum_losses = []\n",
    "            correct_predictions = 0\n",
    "            loss_fn = nn.CrossEntropyLoss().to(self.device)\n",
    "            tmp_eval_accuracy, eval_accuracy = 0, 0\n",
    "                \n",
    "            with torch.no_grad():\n",
    "                for batch in test_loader:\n",
    "                    \n",
    "                    input_ids = batch['input_ids'].to(device)\n",
    "                    input_mask = batch['attention_mask'].to(device)\n",
    "                    labels = batch['targets'].to(device)\n",
    "\n",
    "                    outputs = self.model(input=input_ids, attention_mask=input_mask, classification=True)\n",
    "                    \n",
    "                    loss = clf_loss_func(input=outputs, target=labels)\n",
    "\n",
    "                    correct_predictions += torch.sum(torch.max(outputs, dim=1) == labels)\n",
    "                    sum_losses.append(loss)\n",
    "                        \n",
    "            print('\\nTest set: loss: {:.4f}, Accuracy: {:.0f}%\\n'.format(\n",
    "                    np.mean(sum_losses), 100. * correct_predictions.double() / len(test_loader.dataset)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MatML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
