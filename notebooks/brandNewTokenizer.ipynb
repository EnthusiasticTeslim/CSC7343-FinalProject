{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class AntigenTCRDataset(Dataset):\n",
    "    def __init__(self, csv_file, total_max_length, CLS='[CLS]', SEP='[SEP]', PAD='[PAD]', UNK='[UNK]'):\n",
    "        # Read the CSV file\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        \n",
    "        # Special tokens\n",
    "        self.CLS = CLS\n",
    "        self.SEP = SEP\n",
    "        self.PAD = PAD\n",
    "        self.UNK = UNK\n",
    "\n",
    "        # Encoding dictionary for amino acids and special tokens\n",
    "        self.encodings = {\n",
    "            f\"{self.PAD}\": 0, f\"{self.CLS}\": 1, f\"{self.SEP}\": 2, \"[MASK]\": 3,\n",
    "            f\"{self.UNK}\": 4, \"L\": 5, \"W\": 6, \"H\": 7, \"N\": 8, \"R\": 9, \"S\": 10,\n",
    "            \"M\": 11, \"D\": 12, \"A\": 13, \"Q\": 14, \"C\": 15, \"F\": 16, \"V\": 17,\n",
    "            \"K\": 18, \"G\": 19, \"I\": 20, \"E\": 21, \"Y\": 22, \"P\": 23, \"T\": 24\n",
    "        }\n",
    "\n",
    "        # Process and pad the sequences\n",
    "        self.data['combined_seqs'] = self.data.apply(lambda row: self.pad_combined_sequences(row['antigen'], row['TCR'], total_max_length), axis=1)\n",
    "\n",
    "        # Convert to input tokens and attention masks\n",
    "        self.input_tokens = [self.sequence_to_input_tokens(seq, self.encodings) for seq in self.data['combined_seqs']]\n",
    "        self.attention_masks = [self.create_attention_mask(tokens, self.encodings) for tokens in self.input_tokens]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def separate_aa(self, sequence):\n",
    "        return ' '.join(sequence)\n",
    "\n",
    "    def pad_combined_sequences(self, antigen_sequence, tcr_sequence, total_max_length):\n",
    "        # Separate amino acids in each sequence\n",
    "        separated_antigen = self.separate_aa(antigen_sequence)\n",
    "        separated_tcr = self.separate_aa(tcr_sequence)\n",
    "\n",
    "        # Combine sequences\n",
    "        combined = f'{self.CLS} {separated_antigen} {self.SEP} {separated_tcr}'\n",
    "        combined_length = len(combined.replace(' ', ''))  # Count characters excluding spaces\n",
    "\n",
    "        # Calculate the needed padding and apply it\n",
    "        padding_length = total_max_length - combined_length\n",
    "        padding = ' '.join([self.PAD] * padding_length)\n",
    "        if padding:  # Add a leading space if padding is not empty\n",
    "            padding = ' ' + padding\n",
    "\n",
    "        combined += padding\n",
    "        return combined\n",
    "\n",
    "    def sequence_to_input_tokens(self, sequence, encodings):\n",
    "        return [encodings.get(elem, encodings[self.UNK]) for elem in sequence.split()]\n",
    "\n",
    "    def create_attention_mask(self, input_tokens, encodings):\n",
    "        return [1 if token != encodings[self.PAD] else 0 for token in input_tokens]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        sample = {\n",
    "            'antigen': self.data.iloc[idx]['antigen'],\n",
    "            'tcr': self.data.iloc[idx]['TCR'],\n",
    "            'combined_sequences': self.data.iloc[idx]['combined_seqs'],\n",
    "            'input_ids': torch.tensor(self.input_tokens[idx]),\n",
    "            'attention_mask': torch.tensor(self.attention_masks[idx])\n",
    "        }\n",
    "\n",
    "        return sample\n",
    "\n",
    "# Example usage\n",
    "# Note: You need to specify 'total_max_length' based on your data\n",
    "dataset = AntigenTCRDataset('../data/data_balanced.csv', total_max_length=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {f\"{self.PAD}\": 0, f\"{self.CLS}\": 1, f\"{self.SEP}\": 2, \"[MASK]\": 3, \n",
    "#                             f\"{self.UNK}\": 4, \"L\": 5, \"W\": 6, \"H\": 7, \"N\": 8, \"R\": 9, \"S\": 10, \n",
    "#                             \"M\": 11, \"D\": 12, \"A\": 13, \"Q\": 14, \"C\": 15, \"F\": 16, \"V\": 17, \n",
    "#                             \"K\": 18, \"G\": 19, \"I\": 20, \"E\": 21, \"Y\": 22, \"P\": 23, \"T\": 24}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'antigen': 'LLWNGPMAV',\n",
       " 'tcr': 'CASSPIGGATDTQYF',\n",
       " 'combined_sequences': '[CLS] L L W N G P M A V [SEP] C A S S P I G G A T D T Q Y F [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
       " 'input_ids': tensor([ 1,  5,  5,  6,  8, 19, 23, 11, 13, 17,  2, 15, 13, 10, 10, 23, 20, 19,\n",
       "         19, 13, 24, 12, 24, 14, 22, 16,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: 42, attention_mask: 42\n",
      "input_ids: 42, attention_mask: 42\n",
      "input_ids: 42, attention_mask: 42\n",
      "input_ids: 42, attention_mask: 42\n",
      "input_ids: 42, attention_mask: 42\n",
      "input_ids: 42, attention_mask: 42\n",
      "input_ids: 42, attention_mask: 42\n",
      "input_ids: 42, attention_mask: 42\n",
      "input_ids: 42, attention_mask: 42\n",
      "input_ids: 42, attention_mask: 42\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(f\"input_ids: {len(dataset[i]['input_ids'])}, attention_mask: {len(dataset[i]['attention_mask'])}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CSC7343",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
