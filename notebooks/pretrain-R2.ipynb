{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForMaskedLM, DataCollatorForLanguageModeling, Trainer,  TrainingArguments\n",
    "from transformers import BertModel, BertConfig\n",
    "\n",
    "import datasets\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./data/data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.drop(columns=['interaction'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply a lambda function to insert spaces between characters\n",
    "data['antigen'] = data['antigen'].apply(lambda x: ' '.join(list(x)))\n",
    "data['TCR'] = data['TCR'].apply(lambda x: ' '.join(list(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Tokenize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_CONFIG = BertConfig(\n",
    "    vocab_size=25,\n",
    "    max_position_embeddings=64,\n",
    "    type_vocab_size=2,\n",
    "    num_attention_heads=8,\n",
    "    num_hidden_layers=8,\n",
    "    hidden_size=512,\n",
    "    intermediate_size=2048,\n",
    "    num_labels=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = BERT_CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"src/antigen\", config=config)\n",
    "tokenizer.model_max_length = 64\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[CLS]antigen[SEP]TCR[EOS]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from datasets import Dataset as HFDataset  # Importing Hugging Face Dataset as HFDataset to avoid confusion with PyTorch Dataset\n",
    "from transformers import PreTrainedTokenizerBase  # Assuming you're using a tokenizer from the transformers library\n",
    "\n",
    "file='./data/data.csv'\n",
    "\n",
    "data = pd.read_csv(file)\n",
    "# Apply a lambda function to insert spaces between characters\n",
    "data['antigen'] = data['antigen'].apply(lambda x: ' '.join(list(x)))\n",
    "data['TCR'] = data['TCR'].apply(lambda x: ' '.join(list(x)))\n",
    "\n",
    "# Put into Hugging Face dataset\n",
    "dataset = HFDataset.from_pandas(data)\n",
    "dataset = dataset.train_test_split(test_size=0.2)\n",
    "\n",
    "max_len = 64\n",
    "column_names = data.columns.tolist()\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[column_names[0]], examples[column_names[1]], max_length=max_len, padding='max_length', truncation=True, return_tensors=\"pt\")\n",
    "        \n",
    "tokenized_datasets = dataset.map(\n",
    "            tokenize_function,\n",
    "            batched=True,\n",
    "            remove_columns=column_names[:2],\n",
    "            desc=\"Running tokenizer on dataset\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for j in range(10):\n",
    "#     print(len(tokenized_datasets['train']['input_ids'][j]), len(tokenized_datasets['train']['attention_mask'][j]), (tokenized_datasets['train']['interaction'][j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.classifier import ModelTrainer\n",
    "import sys\n",
    "import numpy as np\n",
    "import logging\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from src.model import FocalLoss, TCRModel # model, loss function\n",
    "\n",
    "# \"\"\"\n",
    "#     package versions:\n",
    "#         torch: 2.1.1+cu121\n",
    "#         transformers: 4.35.2\n",
    "#         sklearn: 1.3.0\n",
    "#         logging: 0.5.1.2\n",
    "# \"\"\"\n",
    "\n",
    "\n",
    "# key reference: \n",
    "#               https://github.com/aws-samples/amazon-sagemaker-protein-classification/blob/main/code/train.py\n",
    "#               https://medium.com/analytics-vidhya/bert-pre-training-fine-tuning-eb574be614f6\n",
    "class ModelTrainer(nn.Module):\n",
    "\n",
    "    \"\"\"\n",
    "        ************** Train/Test the model using cross validation ************** \n",
    "        seed: seed for random number generator\n",
    "        epochs: number of epochs to train\n",
    "        lr: learning rate\n",
    "        train: flag whether to train the model\n",
    "        log_interval: how many batches to wait before logging training status\n",
    "        model: takes input_ids: str, attention_mask: str, classification: bool\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, train=True, seed = 2023, lr=2e-5, epochs=1000, log_interval=10):\n",
    "        super(ModelTrainer, self).__init__()\n",
    "        self.seed = seed \n",
    "        self.epochs = epochs \n",
    "        self.lr = lr    \n",
    "        self.log_interval = log_interval \n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else (\"mps\" if torch.backends.mps.is_available() else \"cpu\"))\n",
    "        self.model = TCRModel().to(self.device)\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "        self.loss_func = FocalLoss(gamma=3, alpha=0.25, no_agg=False, size_average=True)    \n",
    "\n",
    "    def validate(self, val_loader, model, loss_func):\n",
    "        \"\"\"Evaluate the network on the entire validation (part of training data) set.\"\"\"\n",
    "\n",
    "        loss_accum = []\n",
    "        model.eval()\n",
    "        with torch.set_grad_enabled(False):\n",
    "\n",
    "            for data in val_loader:\n",
    "\n",
    "                input_ids = data['input_ids'].to(self.device)\n",
    "                input_mask = data['attention_mask'].to(self.device)\n",
    "                labels = data['interaction'].to(self.device)\n",
    "\n",
    "                outputs = model(input_ids=input_ids, attention_mask=input_mask)\n",
    "\n",
    "                loss = loss_func(input=outputs, target=labels)\n",
    "                loss_accum.append(loss.item())\n",
    "\n",
    "\n",
    "        return np.mean(loss_accum)  \n",
    "\n",
    "    def test(self, test_loader, model, loss_func):\n",
    "        \"\"\"Evaluate the network on the entire test set.\"\"\"\n",
    "            \n",
    "        model.eval()\n",
    "        sum_losses = []\n",
    "        correct_predictions = 0\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            for data in test_loader:\n",
    "                    \n",
    "                input_ids = data['input_ids'].to(self.device)\n",
    "                input_mask = data['attention_mask'].to(self.device)\n",
    "                labels = data['interaction'].to(self.device)\n",
    "\n",
    "                outputs = model(input_ids=input_ids, attention_mask=input_mask)\n",
    "                    \n",
    "                loss = loss_func(input=outputs, target=labels)\n",
    "\n",
    "                correct_predictions += torch.sum(torch.max(outputs, dim=1) == labels)\n",
    "                sum_losses.append(loss.item())\n",
    "                        \n",
    "            print('\\nTest set: loss: {:.4f}, Accuracy: {:.0f}%\\n'.format(\n",
    "                    np.mean(sum_losses), 100. * correct_predictions.double() / len(test_loader.dataset)))\n",
    "\n",
    "    def train(self, train_loader, test_loader, fold = 3, batch_size = 32):\n",
    "        '''Train the network on the training set using cross validation.'''\n",
    "\n",
    "        print(f\"Training on: {self.device}\")\n",
    "\n",
    "        torch.manual_seed(self.seed) # set the seed for generating random numbers\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed(self.seed)\n",
    "        \n",
    "        # split data for K-fold cross validation to avoid overfitting\n",
    "        indices = list(range(len(train_loader.dataset)))\n",
    "        kf = KFold(n_splits=fold, shuffle=True)\n",
    "\n",
    "        for cv_index, (train_indices, valid_indices) in enumerate(kf.split(indices)):\n",
    "\n",
    "            train_sampler = SubsetRandomSampler(train_indices)\n",
    "            valid_sampler = SubsetRandomSampler(valid_indices)\n",
    "\n",
    "            train_loader = DataLoader(train_loader.dataset, batch_size=batch_size,\n",
    "                                                       sampler=train_sampler,\n",
    "                                                       shuffle=False, collate_fn=collate_fn, pin_memory=True)\n",
    "            val_loader = DataLoader(train_loader.dataset, batch_size=batch_size,\n",
    "                                                     sampler=valid_sampler,\n",
    "                                                     shuffle=False, collate_fn=collate_fn, pin_memory=True)\n",
    "            epoch_train_loss = []\n",
    "            for epoch in range(0, self.epochs + 1):\n",
    "\n",
    "                self.model.train()\n",
    "                \n",
    "                for data in train_loader:\n",
    "\n",
    "                    #print(data['input_ids'])\n",
    "\n",
    "                    input_ids = data['input_ids'].to(self.device)   # amino acid index numbers\n",
    "                    input_mask = data['attention_mask'].to(self.device) # attention mask (1 for non-padding token and 0 for padding)\n",
    "                    labels = data['interaction'].to(self.device) # True for classification task\n",
    "\n",
    "                    outputs = self.model(\n",
    "                                        input_ids = input_ids, attention_mask = input_mask)\n",
    "                 \n",
    "                    loss = self.loss_func(input=outputs, target=labels)\n",
    "                   \n",
    "                    epoch_train_loss.append(loss.item())\n",
    "\n",
    "                    loss.backward()\n",
    "                    self.optimizer.step()\n",
    "                    self.optimizer.zero_grad()\n",
    "                    \n",
    "\n",
    "                # train & validation error after every epoch\n",
    "                #print(epoch_train_loss)\n",
    "                train_loss_avg = np.mean(epoch_train_loss)\n",
    "                val_loss_avg = self.validate(val_loader=val_loader, model=self.model, loss_func=self.loss_func)\n",
    "\n",
    "                print(\"At end of Epoch: {}/{}, Training Loss: {:.4f},  Validation Loss: {:.4f}\".format(\n",
    "                                epoch, self.epochs, train_loss_avg, val_loss_avg))\n",
    "                \n",
    "            # model testing\n",
    "            print('Testing the model...')\n",
    "            self.test(test_loader=test_loader, model=self.model, loss_func=self.loss_func)\n",
    "            print('Finished training & testing the model.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return {key: torch.stack([torch.tensor(val[key]) for val in batch]) for key in batch[0]}\n",
    "\n",
    "batch_size = 512\n",
    "train_loader = torch.utils.data.DataLoader(tokenized_datasets['train'], batch_size=batch_size, shuffle=True, collate_fn=collate_fn, pin_memory=True)\n",
    "test_loader = torch.utils.data.DataLoader(tokenized_datasets['test'], batch_size=batch_size, shuffle=True, collate_fn=collate_fn, pin_memory=True)\n",
    "# model\n",
    "Model = ModelTrainer(epochs=10, lr=1e-3)\n",
    "Model.train(train_loader=train_loader, test_loader=test_loader, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
