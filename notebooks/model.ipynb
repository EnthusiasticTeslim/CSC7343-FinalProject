{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForMaskedLM, DataCollatorForLanguageModeling, Trainer,  TrainingArguments\n",
    "import datasets\n",
    "import evaluate\n",
    "\n",
    "dataset = datasets.load_dataset('csv', data_files='../data/data.csv', split=\"all\")\n",
    "\n",
    "dataset = dataset.remove_columns('interaction')\n",
    "\n",
    "dataset = dataset.train_test_split(test_size=0.2)\n",
    "\n",
    "# BERT\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "model = AutoModelForMaskedLM.from_config(config)\n",
    "\n",
    "column_names = list(dataset[\"train\"].features)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[column_names[0]],examples[column_names[1]], return_special_tokens_mask=False, \n",
    "                     padding='longest', truncation='longest_first', return_tensors=\"pt\")\n",
    "\n",
    "tokenized_datasets = dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=column_names,\n",
    "    desc=\"Running tokenizer on every text in dataset\",\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/aws-samples/amazon-sagemaker-protein-classification/blob/main/code/train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, TensorDataset\n",
    "from transformers import BertTokenizer, get_linear_schedule_with_warmup\n",
    "import torch_optimizer as optim\n",
    "\n",
    "# Network definition\n",
    "from model_def import ProteinClassifier\n",
    "from data_prep import ProteinSequenceDataset\n",
    " \n",
    "## SageMaker Distributed code.\n",
    "from smdistributed.dataparallel.torch.parallel.distributed import DistributedDataParallel as DDP\n",
    "import smdistributed.dataparallel.torch.distributed as dist\n",
    "\n",
    "dist.init_process_group()\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.addHandler(logging.StreamHandler(sys.stdout))\n",
    "\n",
    "MAX_LEN = 512  # this is the max length of the sequence\n",
    "PRE_TRAINED_MODEL_NAME = 'Rostlab/prot_bert_bfd_localization'\n",
    "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME, do_lower_case=False)\n",
    "\n",
    "def _get_train_data_loader(batch_size, training_dir):\n",
    "    dataset = pd.read_csv(os.path.join(training_dir, \"deeploc_per_protein_train.csv\"))\n",
    "    train_data = ProteinSequenceDataset(\n",
    "        sequence=dataset.sequence.to_numpy(),\n",
    "        targets=dataset.location.to_numpy(),\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=MAX_LEN\n",
    "  )\n",
    "    train_sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "            dataset,\n",
    "            num_replicas=dist.get_world_size(),\n",
    "            rank=dist.get_rank())\n",
    "    train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True,\n",
    "                                  sampler=train_sampler)\n",
    "    return train_dataloader\n",
    "\n",
    "def _get_test_data_loader(batch_size, training_dir):\n",
    "    dataset = pd.read_csv(os.path.join(training_dir, \"deeploc_per_protein_test.csv\"))\n",
    "    test_data = ProteinSequenceDataset(\n",
    "        sequence=dataset.sequence.to_numpy(),\n",
    "        targets=dataset.location.to_numpy(),\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=MAX_LEN\n",
    "  )\n",
    "    test_sampler = RandomSampler(test_data)\n",
    "    test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n",
    "    return test_dataloader\n",
    "\n",
    "def freeze(model, frozen_layers):\n",
    "    modules = [model.bert.encoder.layer[:frozen_layers]] \n",
    "    for module in modules:\n",
    "        for param in module.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "def train(args):\n",
    "    use_cuda = args.num_gpus > 0\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    \n",
    "    world_size = dist.get_world_size()\n",
    "    rank = dist.get_rank()\n",
    "    local_rank = dist.get_local_rank()\n",
    "    \n",
    "    # set the seed for generating random numbers\n",
    "    torch.manual_seed(args.seed)\n",
    "    if use_cuda:\n",
    "        torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "    train_loader = _get_train_data_loader(args.batch_size, args.data_dir)\n",
    "    if rank == 0:\n",
    "        test_loader = _get_test_data_loader(args.test_batch_size, args.test)\n",
    "        print(\"Max length of sequence: \", MAX_LEN)\n",
    "        print(\"Freezing {} layers\".format(args.frozen_layers))\n",
    "        print(\"Model used: \", PRE_TRAINED_MODEL_NAME)\n",
    "\n",
    "    logger.debug(\n",
    "        \"Processes {}/{} ({:.0f}%) of train data\".format(\n",
    "            len(train_loader.sampler),\n",
    "            len(train_loader.dataset),\n",
    "            100.0 * len(train_loader.sampler) / len(train_loader.dataset),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    model = ProteinClassifier(\n",
    "        args.num_labels  # The number of output labels.\n",
    "    )\n",
    "    freeze(model, args.frozen_layers)\n",
    "    model = DDP(model.to(device), broadcast_buffers=False)\n",
    "    torch.cuda.set_device(local_rank)\n",
    "    model.cuda(local_rank)\n",
    "    \n",
    "    optimizer = optim.Lamb(\n",
    "            model.parameters(), \n",
    "            lr = args.lr * dist.get_world_size(), \n",
    "            betas=(0.9, 0.999), \n",
    "            eps=args.epsilon, \n",
    "            weight_decay=args.weight_decay)\n",
    "    \n",
    "    total_steps = len(train_loader.dataset)\n",
    "    \n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=total_steps)\n",
    "    \n",
    "    loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "    \n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        model.train()\n",
    "        for step, batch in enumerate(train_loader):\n",
    "            b_input_ids = batch['input_ids'].to(device)\n",
    "            b_input_mask = batch['attention_mask'].to(device)\n",
    "            b_labels = batch['targets'].to(device)\n",
    "\n",
    "            outputs = model(b_input_ids,attention_mask=b_input_mask)\n",
    "            loss = loss_fn(outputs, b_labels)\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            # modified based on their gradients, the learning rate, etc.\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            if step % args.log_interval == 0 and rank == 0:\n",
    "                logger.info(\n",
    "                    \"Collecting data from Master Node: \\n Train Epoch: {} [{}/{} ({:.0f}%)] Training Loss: {:.6f}\".format(\n",
    "                        epoch,\n",
    "                        step * len(batch['input_ids'])*world_size,\n",
    "                        len(train_loader.dataset),\n",
    "                        100.0 * step / len(train_loader),\n",
    "                        loss.item(),\n",
    "                    )\n",
    "                )\n",
    "            if args.verbose:\n",
    "                print('Batch', step, \"from rank\", rank)\n",
    "        if rank == 0:\n",
    "            test(model, test_loader, device)\n",
    "        scheduler.step()\n",
    "    if rank == 0:\n",
    "        model_save = model.module if hasattr(model, \"module\") else model\n",
    "        save_model(model_save, args.model_dir)\n",
    "\n",
    "def save_model(model, model_dir):\n",
    "    path = os.path.join(model_dir, 'model.pth')\n",
    "    # recommended way from http://pytorch.org/docs/master/notes/serialization.html\n",
    "    torch.save(model.state_dict(), path)\n",
    "    logger.info(f\"Saving model: {path} \\n\")\n",
    "\n",
    "def test(model, test_loader, device):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "    tmp_eval_accuracy, eval_accuracy = 0, 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            b_input_ids = batch['input_ids'].to(device)\n",
    "            b_input_mask = batch['attention_mask'].to(device)\n",
    "            b_labels = batch['targets'].to(device)\n",
    "\n",
    "            outputs = model(b_input_ids,attention_mask=b_input_mask)\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            loss = loss_fn(outputs, b_labels)\n",
    "            correct_predictions += torch.sum(preds == b_labels)\n",
    "            losses.append(loss.item())\n",
    "            \n",
    "    print('\\nTest set: Validation loss: {:.4f}, Validation Accuracy: {:.0f}%\\n'.format(\n",
    "        np.mean(losses),\n",
    "        100. * correct_predictions.double() / len(test_loader.dataset)))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Data and model checkpoints directories\n",
    "    parser.add_argument(\n",
    "        \"--num_labels\", type=int, default=10, metavar=\"N\", help=\"input batch size for training (default: 10)\"\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--batch-size\", type=int, default=1, metavar=\"N\", help=\"input batch size for training (default: 1)\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--test-batch-size\", type=int, default=8, metavar=\"N\", help=\"input batch size for testing (default: 8)\"\n",
    "    )\n",
    "    parser.add_argument(\"--epochs\", type=int, default=2, metavar=\"N\", help=\"number of epochs to train (default: 2)\")\n",
    "    parser.add_argument(\"--lr\", type=float, default=0.3e-5, metavar=\"LR\", help=\"learning rate (default: 0.3e-5)\")\n",
    "    parser.add_argument(\"--weight_decay\", type=float, default=0.01, metavar=\"M\", help=\"weight_decay (default: 0.01)\")\n",
    "    parser.add_argument(\"--seed\", type=int, default=43, metavar=\"S\", help=\"random seed (default: 43)\")\n",
    "    parser.add_argument(\"--epsilon\", type=int, default=1e-8, metavar=\"EP\", help=\"random seed (default: 1e-8)\")\n",
    "    parser.add_argument(\"--frozen_layers\", type=int, default=10, metavar=\"NL\", help=\"number of frozen layers(default: 10)\")\n",
    "    parser.add_argument('--verbose', action='store_true', default=False,help='For displaying SMDataParallel-specific logs')\n",
    "    parser.add_argument(\n",
    "        \"--log-interval\",\n",
    "        type=int,\n",
    "        default=10,\n",
    "        metavar=\"N\",\n",
    "        help=\"how many batches to wait before logging training status\",\n",
    "    )\n",
    "   \n",
    "    # Container environment\n",
    "    parser.add_argument(\"--hosts\", type=list, default=json.loads(os.environ[\"SM_HOSTS\"]))\n",
    "    parser.add_argument(\"--current-host\", type=str, default=os.environ[\"SM_CURRENT_HOST\"])\n",
    "    parser.add_argument(\"--model-dir\", type=str, default=os.environ[\"SM_MODEL_DIR\"])\n",
    "    parser.add_argument(\"--data-dir\", type=str, default=os.environ[\"SM_CHANNEL_TRAINING\"])\n",
    "    parser.add_argument(\"--test\", type=str, default=os.environ[\"SM_CHANNEL_TESTING\"])\n",
    "    parser.add_argument(\"--num-gpus\", type=int, default=os.environ[\"SM_NUM_GPUS\"])\n",
    "\n",
    "    train(parser.parse_args())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CSC7343",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
