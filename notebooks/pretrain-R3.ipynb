{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tolayi1\\Documents\\GitHub\\Project\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.chdir('../')\n",
    "\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import time, random, datasets, evaluate \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from src.classifier import ModelTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(130471, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>antigen</th>\n",
       "      <th>TCR</th>\n",
       "      <th>interaction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A A G I G I L T V</td>\n",
       "      <td>C A I S E V G V G Q P Q H F</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A A G I G I L T V</td>\n",
       "      <td>C A S S L S F G T E A F F</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A A R A V F L A L</td>\n",
       "      <td>C A S L G A Q N N E Q F</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A A R A V F L A L</td>\n",
       "      <td>C A S S Y S T G D E Q Y F</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A I M D K N I I L</td>\n",
       "      <td>C A S S V D G G S Q P Q H F</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             antigen                          TCR  interaction\n",
       "0  A A G I G I L T V  C A I S E V G V G Q P Q H F            1\n",
       "1  A A G I G I L T V    C A S S L S F G T E A F F            1\n",
       "2  A A R A V F L A L      C A S L G A Q N N E Q F            1\n",
       "3  A A R A V F L A L    C A S S Y S T G D E Q Y F            1\n",
       "4  A I M D K N I I L  C A S S V D G G S Q P Q H F            1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## load data\n",
    "data = pd.read_csv('./data/data.csv')\n",
    "\n",
    "# Calculate the maximum length for the sequences\n",
    "max_antigen_len = max([len(x) for x in data['antigen']])\n",
    "max_TCR_len = max([len(x) for x in data['TCR']])\n",
    "\n",
    "# Apply a lambda function to insert spaces between characters\n",
    "data['antigen'] = data['antigen'].apply(lambda x: ' '.join(list(x)))\n",
    "data['TCR'] = data['TCR'].apply(lambda x: ' '.join(list(x)))\n",
    "\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 20)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_antigen_len, max_TCR_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Tokenize Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column names: ['antigen', 'TCR', 'interaction']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00ff6d4391f34d31ab9987bfa9bef860",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/130471 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from datasets import Dataset as HFDataset  # Importing Hugging Face Dataset as HFDataset to avoid confusion with PyTorch Dataset\n",
    "from src.model import BERT_CONFIG\n",
    "\n",
    "## Tokenizer data\n",
    "config = BERT_CONFIG\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"antigen\", config=config)\n",
    "tokenizer.model_max_length = 64\n",
    "\n",
    "# Put into Hugging Face dataset\n",
    "dataset = HFDataset.from_pandas(data)\n",
    "#dataset = dataset.train_test_split(test_size=0.2)\n",
    "\n",
    "column_names = data.columns.tolist()\n",
    "\n",
    "print(f\"column names: {column_names}\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[column_names[0]], examples[column_names[1]], return_special_tokens_mask=False,\n",
    "                     padding='longest', truncation='longest_first', return_tensors=\"pt\")\n",
    "\n",
    "tokenized_datasets = dataset.map(\n",
    "            tokenize_function,\n",
    "            batched=True,\n",
    "            #remove_columns=column_names[:2],\n",
    "            desc=\"Running tokenizer on dataset\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['antigen', 'TCR', 'interaction', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 130471\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 14, 14, 5, 12, 5, 12, 16, 6, 23, 2, 7, 14, 12, 17, 11, 23, 5, 23, 5, 8, 24, 8, 21, 20, 2, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "1\n",
      "33\n",
      "('A A G I G I L T V', 'C A I S E V G V G Q P Q H F', 1)\n"
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "print(tokenized_datasets[idx]['input_ids'])\n",
    "print(tokenized_datasets[idx]['attention_mask'])\n",
    "print(tokenized_datasets[idx]['interaction'])\n",
    "print(len(tokenized_datasets[idx]['input_ids']))\n",
    "\n",
    "print(f\"{tokenized_datasets[idx]['antigen'], tokenized_datasets[idx]['TCR'], tokenized_datasets[idx]['interaction']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] A A G I G I L T V [SEP] C A I S E V G V G Q P Q H F [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenized_datasets[idx][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column names: ['antigen', 'TCR', 'interaction']\n",
      "max_len: 34\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ee5d799f15145c59892542634b75ba9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/130471 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Tokenizer data\n",
    "config = BERT_CONFIG\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"antigen\", config=config)\n",
    "tokenizer.model_max_length = 64\n",
    "\n",
    "# Put into Hugging Face dataset\n",
    "dataset = HFDataset.from_pandas(data)\n",
    "#dataset = dataset.train_test_split(test_size=0.2)\n",
    "\n",
    "column_names = data.columns.tolist()\n",
    "\n",
    "print(f\"column names: {column_names}\")\n",
    "\n",
    "# 3 for [CLS], [SEP], [PAD] and spaces between characters in the sequences\n",
    "max_len = max_antigen_len + max_TCR_len + 3\n",
    "print(f\"max_len: {max_len}\")\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[column_names[0]], examples[column_names[1]], max_length=max_len, padding='max_length', truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "tokenized_datasets = dataset.map(\n",
    "            tokenize_function,\n",
    "            batched=True,\n",
    "            #remove_columns=column_names[:2],\n",
    "            desc=\"Running tokenizer on dataset\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['antigen', 'TCR', 'interaction', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 130471\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 14, 16, 13, 5, 17, 23, 24, 23, 16, 2, 7, 14, 17, 17, 8, 5, 5, 5, 5, 6, 19, 6, 8, 13, 20, 2, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "1\n",
      "34\n",
      "('A L Y G S V P V L', 'C A S S Q G G G G T D T Q Y F', 1)\n"
     ]
    }
   ],
   "source": [
    "idx = 50\n",
    "print(tokenized_datasets[idx]['input_ids'])\n",
    "print(tokenized_datasets[idx]['attention_mask'])\n",
    "print(tokenized_datasets[idx]['interaction'])\n",
    "print(len(tokenized_datasets[idx]['input_ids']))\n",
    "\n",
    "print(f\"{tokenized_datasets[idx]['antigen'], tokenized_datasets[idx]['TCR'], tokenized_datasets[idx]['interaction']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] A L Y G S V P V L [SEP] C A S S Q G G G G T D T Q Y F [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenized_datasets[idx][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dataPreprocessing import AntigenTCRDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130471\n"
     ]
    }
   ],
   "source": [
    "dataset = AntigenTCRDataset('./data/data.csv')\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1, 14, 16, 13,  5, 17, 23, 24, 23, 16,  2,  7, 14, 17, 17,  8,  5,  5,\n",
      "         5,  5,  6, 19,  6,  8, 13, 20,  2,  0,  0,  0,  0,  0,  0,  0])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor(1)\n",
      "34 34\n",
      "ALYGSVPVL CASSQGGGGTDTQYF\n",
      "[CLS] A L Y G S V P V L [SEP] C A S S Q G G G G T D T Q Y F [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "idx = 50\n",
    "print(dataset[idx]['input_ids'])\n",
    "print(dataset[idx]['attention_mask'])\n",
    "print(dataset[idx]['interaction'])\n",
    "print(len(dataset[idx]['input_ids']), len(dataset[idx]['attention_mask']))\n",
    "print(dataset[idx]['antigen'], dataset[idx]['tcr'])\n",
    "print(dataset[idx]['combined_sequences'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(104376, 26095)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset, test_dataset = dataset.split_data(test_size=0.2, state=48)\n",
    "len(train_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34, 34, tensor(1))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset[0]['input_ids']), len(train_dataset[0]['attention_mask']), (train_dataset[0]['interaction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1, 11, 16,  ...,  0,  0,  0],\n",
      "        [ 1, 16, 16,  ...,  0,  0,  0],\n",
      "        [ 1, 20, 15,  ...,  0,  0,  0],\n",
      "        ...,\n",
      "        [ 1, 20, 16,  ...,  0,  0,  0],\n",
      "        [ 1, 22, 16,  ...,  0,  0,  0],\n",
      "        [ 1,  9, 16,  ...,  0,  0,  0]])\n",
      "[34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34]\n",
      "tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])\n",
      "[34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34]\n",
      "tensor([0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 1, 1, 1, 0, 0, 0, 1])\n",
      "Length: @input_ids - 32, @attention_mask - 32, @interaction - 32\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# # Step 3: Create a DataLoader\n",
    "dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Iterate Over the DataLoader\n",
    "for batch in dataloader:\n",
    "    batch_input_ids = batch['input_ids']\n",
    "    batch_attention_mask = batch['attention_mask']\n",
    "    batch_label = batch['interaction']\n",
    "    print(batch_input_ids)\n",
    "    print([len(item) for item in batch_input_ids])\n",
    "    print(batch_attention_mask)\n",
    "    print([len(item) for item in batch_attention_mask])\n",
    "    print(batch_label)\n",
    "    print(f'Length: @input_ids - {len(batch_input_ids)}, @attention_mask - {len(batch_attention_mask)}, @interaction - {len(batch_label)}')\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104376"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataloader.sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[CLS]antigen[SEP]TCR[EOS]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.classifier import ModelTrainer\n",
    "import sys\n",
    "import numpy as np\n",
    "import logging\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from src.model import clf_loss_func, TCRModel # model, loss function\n",
    "\n",
    "# \"\"\"\n",
    "#     package versions:\n",
    "#         torch: 2.1.1+cu121\n",
    "#         transformers: 4.35.2\n",
    "#         sklearn: 1.3.0\n",
    "#         logging: 0.5.1.2\n",
    "# \"\"\"\n",
    "\n",
    "# key reference: \n",
    "#               https://github.com/aws-samples/amazon-sagemaker-protein-classification/blob/main/code/train.py\n",
    "#               https://medium.com/analytics-vidhya/bert-pre-training-fine-tuning-eb574be614f6\n",
    "#               https://medium.com/dataseries/k-fold-cross-validation-with-pytorch-and-sklearn-d094aa00105f\n",
    "class ModelTrainer(nn.Module):\n",
    "\n",
    "    \"\"\"\n",
    "        ************** Train/Test the model using cross validation ************** \n",
    "        seed: seed for random number generator\n",
    "        epochs: number of epochs to train\n",
    "        lr: learning rate\n",
    "        train: flag whether to train the model\n",
    "        log_interval: how many batches to wait before logging training status\n",
    "        model: takes input_ids: str, attention_mask: str, classification: bool\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model=TCRModel(), seed = 2023, lr=2e-5, epochs=1000, log_interval=10):\n",
    "        super(ModelTrainer, self).__init__()\n",
    "        self.seed = seed \n",
    "        self.epochs = epochs \n",
    "        self.lr = lr    \n",
    "        self.log_interval = log_interval \n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else (\"mps\" if torch.backends.mps.is_available() else \"cpu\"))\n",
    "        self.model = model.to(self.device)\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "        self.loss_func = clf_loss_func #FocalLoss(gamma=3, alpha=0.25, no_agg=True)    \n",
    "\n",
    "    def validate(self, val_loader, model, device, loss_func):\n",
    "        \"\"\"Evaluate the network on the entire validation (part of training data) set.\"\"\"\n",
    "\n",
    "        val_loss, val_accuracy = 0, 0\n",
    "        all_labels = []\n",
    "        all_predictions = []\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "\n",
    "            for data in val_loader:\n",
    "                # get the inputs\n",
    "                input_ids = data['input_ids'].to(device)\n",
    "                input_mask = data['attention_mask'].to(device)\n",
    "                labels = data['interaction'].to(device)\n",
    "                # forward pass\n",
    "                outputs = model(input_ids=input_ids, attention_mask=input_mask)\n",
    "                # loss and accuracy\n",
    "                loss = loss_func(input=outputs, target=labels)\n",
    "                val_loss += loss.sum().item() * input_ids.size(0)\n",
    "                scores, predictions = torch.max(outputs, dim=1)\n",
    "                val_accuracy += (predictions == labels).sum().item()\n",
    "\n",
    "                # Store predictions and labels for AUC calculation\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                all_predictions.extend(outputs.cpu().detach().numpy()[:, 1])\n",
    "            \n",
    "        # Compute AUC\n",
    "        auc = roc_auc_score(all_labels, all_predictions)\n",
    "\n",
    "\n",
    "        return val_loss, val_accuracy, auc\n",
    "\n",
    "    def test(self, test_loader, model, loss_func, device):\n",
    "        \"\"\"Evaluate the network on the entire test set and calculate AUC.\"\"\"\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        test_loss, test_accuracy = 0, 0\n",
    "        all_labels = []\n",
    "        all_predictions = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for data in test_loader:\n",
    "                # get the inputs\n",
    "                input_ids = data['input_ids'].to(device)\n",
    "                input_mask = data['attention_mask'].to(device)\n",
    "                labels = data['interaction'].to(device)\n",
    "\n",
    "                # forward pass\n",
    "                outputs = model(input_ids=input_ids, attention_mask=input_mask)\n",
    "\n",
    "                # loss and accuracy\n",
    "                loss = loss_func(input=outputs, target=labels)\n",
    "                test_loss += loss.sum().item() * input_ids.size(0)\n",
    "\n",
    "                scores, predictions = torch.max(outputs, dim=1)\n",
    "                test_accuracy += (predictions == labels).sum().item()\n",
    "\n",
    "                # Store predictions and labels for AUC calculation\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                all_predictions.extend(outputs.cpu().detach().numpy()[:, 1])  # Assuming binary classification (1 is the positive class)\n",
    "\n",
    "        # Compute AUC\n",
    "        auc = roc_auc_score(all_labels, all_predictions)\n",
    "\n",
    "        return test_loss, test_accuracy, auc                  \n",
    "    \n",
    "\n",
    "    def train(self, model, train_loader, loss_func, optimizer, device):\n",
    "        \"\"\"Train the network on the training set.\"\"\"\n",
    "        train_loss, train_accuracy = 0, 0\n",
    "        all_labels = []\n",
    "        all_predictions = []\n",
    "\n",
    "        model.train()\n",
    "        \n",
    "        for data in train_loader:\n",
    "            \n",
    "            # get the inputs\n",
    "            input_ids = data['input_ids'].to(device)   # amino acid index numbers\n",
    "            input_mask = data['attention_mask'].to(device) # attention mask (1 for non-padding token and 0 for padding)\n",
    "            labels = data['interaction'].to(device) # True for classification task\n",
    "            # forward pass\n",
    "            outputs = self.model(input_ids = input_ids, attention_mask = input_mask)\n",
    "            # loss and backward pass\n",
    "            loss = self.loss_func(input=outputs, target=labels)\n",
    "            loss.mean().backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            # loss and accuracy\n",
    "            train_loss += loss.sum().item() * input_ids.size(0)\n",
    "            scores, predictions = torch.max(outputs, dim=1)\n",
    "            train_accuracy += (predictions == labels).sum().item()\n",
    "\n",
    "            # auc score\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_predictions.extend(outputs.cpu().detach().numpy()[:, 1])\n",
    "\n",
    "        # Compute AUC\n",
    "        auc = roc_auc_score(all_labels, all_predictions)\n",
    "\n",
    "        return train_loss, train_accuracy, auc\n",
    "\n",
    "    def execute_run(self, train_loader, test_loader, fold = 3, batch_size = 32):\n",
    "        '''Train, Test and Validate the network on the training set using cross validation.'''\n",
    "\n",
    "        print(f\"Training on: {self.device}\")\n",
    "\n",
    "        torch.manual_seed(self.seed) # set the seed for generating random numbers\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed(self.seed)\n",
    "        \n",
    "        # split data for K-fold cross validation to avoid overfitting\n",
    "        self.fold = fold\n",
    "        indices = list(range(len(train_loader.dataset)))\n",
    "        kf = KFold(n_splits=self.fold, shuffle=True)\n",
    "\n",
    "        for cv_index, (train_indices, valid_indices) in enumerate(kf.split(indices)):\n",
    "\n",
    "            train_sampler = SubsetRandomSampler(train_indices)\n",
    "            valid_sampler = SubsetRandomSampler(valid_indices)\n",
    "\n",
    "            train_loader = DataLoader(train_loader.dataset, batch_size=batch_size,\n",
    "                                                       sampler=train_sampler,\n",
    "                                                       shuffle=False, pin_memory=True)\n",
    "            val_loader = DataLoader(train_loader.dataset, batch_size=batch_size,\n",
    "                                                     sampler=valid_sampler,\n",
    "                                                     shuffle=False, pin_memory=True)\n",
    "            \n",
    "            print(\"CV: {}\".format(cv_index))\n",
    "\n",
    "            self.history = {'train_loss': [], 'val_loss': [],'train_acc':[],'val_acc':[]}\n",
    "\n",
    "            for epoch in range(0, self.epochs + 1):\n",
    "                # Training\n",
    "                epoch_train_loss, epoch_train_accuracy, auc_train = self.train(model=self.model, \n",
    "                                                                    train_loader=train_loader, loss_func=self.loss_func, \n",
    "                                                                    optimizer=self.optimizer, device=self.device)\n",
    "                # Validation\n",
    "                epoch_val_loss, epoch_val_accuracy, auc_val = self.validate(val_loader=val_loader, \n",
    "                                                                    model=self.model, loss_func=self.loss_func, \n",
    "                                                                    device=self.device)\n",
    "                # \n",
    "                train_loss = epoch_train_loss / len(train_loader.sampler)\n",
    "                train_accuracy = epoch_train_accuracy * 100 / len(train_loader.sampler)\n",
    "                val_loss = epoch_val_loss / len(val_loader.sampler)\n",
    "                val_accuracy = epoch_val_accuracy * 100/ len(val_loader.sampler)\n",
    "\n",
    "                self.history['train_loss'].append(train_loss)    \n",
    "                self.history['train_acc'].append(train_accuracy)\n",
    "                self.history['val_loss'].append(val_loss)\n",
    "                self.history['val_acc'].append(val_accuracy)\n",
    "\n",
    "                # train & validation error after every epoch\n",
    "                print(\"Epoch: {}/{}, Training Loss: {:.4f}, Training Accuracy: {:.2f} %, Train AUC score: {:.2f}, Validation Loss: {:.4f}, Validation Accuracy: {:.2f} %, Validation AUC score: {:.2f}\".format(\n",
    "                                epoch, self.epochs, train_loss, train_accuracy, auc_train, val_loss, val_accuracy, auc_val))\n",
    "        \n",
    "        # model testing\n",
    "        print('Testing the model...')\n",
    "        test_loss, test_accuracy, auc_test = self.test(test_loader=test_loader, model=self.model, loss_func=self.loss_func, device=self.device)\n",
    "        test_loss_, test_accuracy_ = test_loss / len(test_loader.sampler), test_accuracy * 100 / len(test_loader.sampler)\n",
    "\n",
    "        print(\"Test Loss: {:.4f}, Test Accuracy: {:.2f} %, Test AUC score\".format(test_loss_, test_accuracy_, auc_test))\n",
    "        print('Finished training & testing the model.')\n",
    "\n",
    "    def save(self, path):\n",
    "        \"\"\"Save the model to the path specified.\"\"\"\n",
    "        # save model\n",
    "        self.model.save(f\"{path}.pt\")\n",
    "        # save history\n",
    "        avg_train_loss = np.mean(self.history['train_loss'])\n",
    "        avg_val_loss = np.mean(self.history['val_loss'])\n",
    "        avg_train_acc = np.mean(self.history['train_acc'])\n",
    "        avg_val_acc = np.mean(self.history['val_acc'])\n",
    "\n",
    "        print('Performance of {} fold cross validation'.format(self.fold))\n",
    "        print(\"Average Training Loss: {:.4f} \\t Average Val Loss: {:.4f} \\t Average Training Acc: {:.3f} \\t Average Val Acc: {:.3f}\".format(avg_train_loss, avg_val_loss,avg_train_acc,avg_val_acc))  \n",
    "        \n",
    "        np.save(f'{path}_history.npy', self.history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on: cuda\n",
      "CV: 0\n",
      "Epoch: 0/1, Training Loss: 5.3285, Training Accuracy: 49.49 %, Train AUC score: 0.50, Validation Loss: 4.1913, Validation Accuracy: 74.97 %, Train AUC score: 0.49\n",
      "Epoch: 1/1, Training Loss: 4.2680, Training Accuracy: 50.04 %, Train AUC score: 0.50, Validation Loss: 4.1612, Validation Accuracy: 25.03 %, Train AUC score: 0.50\n",
      "CV: 1\n",
      "Epoch: 0/1, Training Loss: 4.2135, Training Accuracy: 50.26 %, Train AUC score: 0.50, Validation Loss: 4.1460, Validation Accuracy: 24.58 %, Train AUC score: 0.50\n",
      "Epoch: 1/1, Training Loss: 4.2034, Training Accuracy: 49.23 %, Train AUC score: 0.50, Validation Loss: 4.1361, Validation Accuracy: 24.58 %, Train AUC score: 0.50\n",
      "CV: 2\n",
      "Epoch: 0/1, Training Loss: 4.1744, Training Accuracy: 51.00 %, Train AUC score: 0.50, Validation Loss: 4.1764, Validation Accuracy: 25.32 %, Train AUC score: 0.50\n",
      "Epoch: 1/1, Training Loss: 4.1758, Training Accuracy: 50.58 %, Train AUC score: 0.50, Validation Loss: 4.1748, Validation Accuracy: 25.32 %, Train AUC score: 0.51\n",
      "Testing the model...\n",
      "Test Loss: 4.1443, Test Accuracy: 24.73 %, Test AUC score\n",
      "Finished training & testing the model.\n",
      "Total time taken: 9.88 mins\n"
     ]
    }
   ],
   "source": [
    "# timer\n",
    "start_time = time.time()\n",
    "# dataset\n",
    "batch_size = 128\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "# model\n",
    "\n",
    "epoch=1\n",
    "Model = ModelTrainer(epochs=epoch, lr=1e-3)\n",
    "Model.execute_run(train_loader=train_loader, test_loader=test_loader, batch_size=batch_size, fold=3)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Total time taken: {round((end_time - start_time)/60, 2)} mins\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
